{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 2 columns):\n",
      "ProjectID        4 non-null object\n",
      "Project_Story    4 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 144.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# I used anaconda prompt for python to download all packages.\n",
    "# Import all libraries.\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "\n",
    "# Import the dataset - it needs to be on the same file as the python document.\n",
    "old_indiegogo = pd.read_excel('Nassia1.xlsx')\n",
    "\n",
    "# Create new dataset which will be used to include the scraped information.\n",
    "new_indiegogo = old_indiegogo.filter(['ProjectID','Project_Title'], axis=1)\n",
    "new_indiegogo.columns = ['ProjectID','Project_Story']\n",
    "new_indiegogo['ProjectID'] = new_indiegogo.ProjectID.astype(str)\n",
    "new_indiegogo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nassi\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: use options instead of chrome_options\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each row of our new data set and by indexing\n",
    "# the project ID, scrap the respective webpage.\n",
    "\n",
    "for index, row in new_indiegogo.iterrows():\n",
    "    \n",
    "    # The ID needs to be a string in order to be identified as part of\n",
    "    # the URL.\n",
    "    project_id = str(row['ProjectID'])\n",
    "    \n",
    "    url = 'https://www.indiegogo.com/projects/' + project_id\n",
    "    \n",
    "    # Open driver by adding the path where its located.\n",
    "    driverPath = 'C:/Users/nassi/Desktop/chromedriver.exe'\n",
    "    \n",
    "    # Create the options of the chrome driver which will be used in\n",
    "    # a later stage for better use.\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # When scraping each webpage do not open the browser.\n",
    "    options.add_argument(\"headless\")\n",
    "    \n",
    "    # Use the above information to define the driver which will scrap the data.\n",
    "    driver = webdriver.Chrome(executable_path=driverPath, chrome_options=options)\n",
    "    \n",
    "    # Wait only 4 seconds.\n",
    "    driver.implicitly_wait(4)\n",
    "    \n",
    "    # Run each unique URL with the driver.\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create beautiful Soup object for scraping.\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # Create object which includes only the information under the \n",
    "    # class: routerContentStory-storyBody. \n",
    "    # Convert to string in order to perform slicing.\n",
    "    content = str(soup.find_all('div', {'class': 'routerContentStory-storyBody'}))\n",
    "    \n",
    "    # Remove all HTML/CSS commands.\n",
    "    while content.find('<')!=-1 and content.find('>')!=-1:\n",
    "        content = content[:content.find('<')] + content[content.find('>') + 1:]\n",
    "    \n",
    "    # Further \"prettify\" by removing unnecessary characters.\n",
    "    content_new = ''.join(content)\n",
    "    content_new = content_new.replace(', ' , '')\n",
    "    content_new = content_new.replace('[' , '')\n",
    "    content_new = content_new.replace(']' , '')\n",
    "    content_new = content_new.replace('  ' , '')\n",
    "    content_new = content_new.replace('\\n' , ' ')\n",
    "    \n",
    "    # Add content to each unique projects information\n",
    "    row['Project_Story'] = content_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ProjectID                                      Project_Story\n",
      "0    261303   Rains In LA tells the cute love story depicte...\n",
      "1    261305   Gosu's overarching goal is to be Seattle's fi...\n",
      "2    261310  FIRST Robotics Competition Team 980 ThunderBot...\n",
      "3    261312   Short Summary My Father never Loved Me… How d...\n"
     ]
    }
   ],
   "source": [
    "print(new_indiegogo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Hard link  ProjectID  \\\n",
      "0  www.indiegogo.com/projects/261303     261303   \n",
      "1  www.indiegogo.com/projects/261305     261305   \n",
      "2  www.indiegogo.com/projects/261310     261310   \n",
      "3  www.indiegogo.com/projects/261312     261312   \n",
      "\n",
      "                               Project_Title  \\\n",
      "0                                Rains In LA   \n",
      "1       Gosu: Seattle's First eSports Lounge   \n",
      "2  FRC Team 980 Thunderbots - FIRST Robotics   \n",
      "3                            A Father's Love   \n",
      "\n",
      "                                    Project_Subtitle Project_Category  \\\n",
      "0  A creative and exciting non profit short film ...             Film   \n",
      "1  Gosu is Seattle's first eSports lounge, with c...           Gaming   \n",
      "2  FIRST Robotics Competition Team 980 ThunderBot...        Education   \n",
      "3  The Most Incredible Way to Spread the Love tha...          Writing   \n",
      "\n",
      "  Project_City Project_Country                    Project_Location  \\\n",
      "0       London              GB              London, United Kingdom   \n",
      "1      Seattle              US  Seattle, Washington, United States   \n",
      "2       Sylmar              US   Sylmar, California, United States   \n",
      "3       Dallas              US        Dallas, Texas, United States   \n",
      "\n",
      "                                       Project_Story  \n",
      "0   Rains In LA tells the cute love story depicte...  \n",
      "1   Gosu's overarching goal is to be Seattle's fi...  \n",
      "2  FIRST Robotics Competition Team 980 ThunderBot...  \n",
      "3   Short Summary My Father never Loved Me… How d...  \n"
     ]
    }
   ],
   "source": [
    "# Add the new column information to the old dataset.\n",
    "old_indiegogo['Project_Story'] = new_indiegogo['Project_Story']\n",
    "print(old_indiegogo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create xlsx document in which the new data will be added\n",
    "writer = pd.ExcelWriter('final_indiegogo.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "old_indiegogo.to_excel(writer)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

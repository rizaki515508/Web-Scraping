{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of steps:\n",
    "# 1. Import existing dataset of indiegogo and create a new dataset which will be used\n",
    "#    to store the scraped information.\n",
    "# 2. By assigning each unique project URL to the chromedriver, beautiful soup scraps\n",
    "#    the information we want. The class which is identified was found by inspecting\n",
    "#    a random indiegogo project webpage - CTRL + SHIFT + I in Chrome. \n",
    "# 3. Iterate the above process for all projects available by using the project ID\n",
    "# 4. Create updated dataset by adding the new information extracted to the old dataset.\n",
    "# 5. Export excel file with the new information.\n",
    "\n",
    "# I used anaconda prompt for python to download all packages first. \n",
    "# Language: Python 3.6.\n",
    "\n",
    "# Import all libraries.\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "\n",
    "# Import the dataset - it needs to be on the same folder as the python document.\n",
    "old_indiegogo = pd.read_excel('Nassia1.xlsx')\n",
    "\n",
    "# Create new dataset which will be used to include the scraped information.\n",
    "new_indiegogo = old_indiegogo.filter(['ProjectID','Project_Title'], axis=1)\n",
    "new_indiegogo.columns = ['ProjectID','Project_Story']\n",
    "new_indiegogo['ProjectID'] = new_indiegogo.ProjectID.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nassi\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: use options instead of chrome_options\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each row of our new data set and by indexing\n",
    "# the project ID, scrap the respective webpage.\n",
    "\n",
    "for index, row in new_indiegogo.iterrows():\n",
    "    \n",
    "    # The ID needs to be a string in order to be identified as part of\n",
    "    # the URL.\n",
    "    project_id = str(row['ProjectID'])\n",
    "    \n",
    "    url = 'https://www.indiegogo.com/projects/' + project_id\n",
    "    \n",
    "    # Open driver by adding the path where its located on the computer.\n",
    "    driverPath = 'C:/Users/nassi/Desktop/chromedriver.exe'\n",
    "    \n",
    "    # Create the options of the chrome driver which will be used in\n",
    "    # a later stage for better use.\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # When scraping each webpage do not open the browser.\n",
    "    options.add_argument(\"headless\")\n",
    "    \n",
    "    # Use the above information to define the driver which will scrap the data.\n",
    "    driver = webdriver.Chrome(executable_path=driverPath, chrome_options=options)\n",
    "    \n",
    "    # Wait only 4 seconds.\n",
    "    driver.implicitly_wait(4)\n",
    "    \n",
    "    # Run each unique URL with the driver.\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create beautiful Soup object for scraping.\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # Create object which includes only the information under the \n",
    "    # class: routerContentStory-storyBody. \n",
    "    # Convert to string in order to perform slicing.\n",
    "    content = str(soup.find_all('div', {'class': 'routerContentStory-storyBody'}))\n",
    "    \n",
    "    # Remove all HTML/CSS commands.\n",
    "    while content.find('<')!=-1 and content.find('>')!=-1:\n",
    "        content = content[:content.find('<')] + content[content.find('>') + 1:]\n",
    "    \n",
    "    # Further \"prettify\" by removing unnecessary characters.\n",
    "    content_new = ''.join(content)\n",
    "    content_new = content_new.replace(', ' , '')\n",
    "    content_new = content_new.replace('[' , '')\n",
    "    content_new = content_new.replace(']' , '')\n",
    "    content_new = content_new.replace('  ' , '')\n",
    "    content_new = content_new.replace('\\n' , ' ')\n",
    "    \n",
    "    # Add content to each unique projects 'Project_Story'.\n",
    "    row['Project_Story'] = content_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column information to the old dataset.\n",
    "old_indiegogo['Project_Story'] = new_indiegogo['Project_Story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create final_indiegogo.xlsx document in which the final data for all projects will be added.\n",
    "writer = pd.ExcelWriter('final_indiegogo.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "old_indiegogo.to_excel(writer)\n",
    "\n",
    "# Output the Excel file.\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
